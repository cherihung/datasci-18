#L13 - 4-13-2017

## NLP

- stemming: to identify and group works with different endings but same words together. 'badly', 'bad' => 'bad', 'bad'
- lemmatization: using grammar rules to group different words with same meaning. 'good', 'best', 'better'

- nltk lib: sent_tokenize, workd_tokenize

#L14 - 4-18-2017

## NLP Part 2

## Naive Bayes
Batch learning vs online learning (aka realtime learning)
so far all the ML algorithm we've used are batch learning. e.g. data.fit()
robotics, for example, uses online learning.

Naive Bayes is an online learning model.

for data, like text, that are vectorized into numerical, we want to use Multinomial anything else is Gaussiance

The major assumption is that there's no collinearity between any columns. but because it doesn't look at the correlation, it runs much faster. and while the assumption is wrong especially in text but given enough dataset (really a big dataset and enough columns), it works!

- what we know: p(word | class)
the probability of the word given the class.
e.g. probability of seeing the word "basketball" in the "sports" class.

- model: p(class | word)
predict the probability of the class given the


## key concepts - cleaning:
- stemming, lemmatization, stop words

## key concepts - analyses:
- pos count, sentiment, NER, word count/Tfidf, topics/LDA

- Count Vectorizer:
use .fit() once and first to create the model, then only use .transform() from then on to train your model
- Tfidf Vectorizer: it counts both term frequency (like count Vectorizer) AND document frequency (how many times in a document) BUT inverse (higher score for less doc frequency)
- Tfidf is more accurate, count Vectorizer is faster.

## slide 8 exercise
- "Lincoln Center" "Hearst Foundation" - closely related as they are almost always appear together in a sentence
- topics: grants to performing arts, Heart foundation supports the arts

## dimensionality reduction
- combine high correlated features into one single feature to reduce complexity. you'll lose nuance but is helpful with NLP problems that need to group highly correlated words or texts together.
- http://setosa.io/ev/principal-component-analysis/

## slide 24 exercise

dimensionality reduction vs regularization:
regularization adds accuracy, won't lose any columns in your dataset. it's a layer on top of your learning model after regularizing all your features. dimensionality reduction, you lose your columns but speed up your analysis. and in the world of text, it'd work better.

## LDA

Unsupervised learning algorithm for text.

#L15 - 04-20-2017

## Time series

- Trend (significant and stable increase or decrease) and/or seasonality (regular repeated pattern)
- seasonality measures cycle. if something's normal vs abnormal
- Use *moving average* to refine noise out of data to a clearer picture of overall trend

#L16 - missed Time trends

#L17 - 04-27-2017
## Databases
http://sqlitebrowser.org/

## Big Data
### NoSQL databases
- suitable for high velocity (changing fast), high variety data, changing schema, sparse data (lots of null values)
- fast storage, slower retrieval than traditional relational/sql databases
- store data as nested dictionary instead of column table structure in relation structure. similar to how python dictionary works.

### Graph Networks
- nodes and edges to represent relationships
- 3 connections = 3 degrees
- kind of like NoSQL in that each node can have a dictionary with data
- Strong time-based usage and analysis purpose e.g. outbreak pattern, cascading failure

- _networkx_ pandas library for graphing network. Some info: density = 1 means every node has a connection, closer to 0 means no connection

## slide 21 exercise
1. denormalized. a table for "leagues" with league name and league id. team table contains league id to match.

## slide 67 exercise
types of database to use with:
yelp comments, IMDB, tracking stars - SQL,
Google maps, tracking visitors, - NoSQL

#L18 - 05-02-2017
### Pickle!! to save your model and vectorizer
- use pickle to save any python object or code
- can be saved to any file extension, not necessarily .pickle

### python pipeline
- Spotify Luigi and airBnB airFlow

### exercise - arrival!

- Acquire data: talk to alien, acquire as many writing samples as possible
- Analysis: raw data => determine most common segments, clustering => train the data by verifying with aliens => build a language dictionary library like NLP to eventually parse what the aliens are talking about, decision tree
