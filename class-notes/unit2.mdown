#L5 - 03-12-2017

## Finding data - ELT Process (extra, transform, load)

[Awesome Public dataset on Github](https://github.com/caesar0301/awesome-public-datasets)
[UCI MI Repo](http://archive.ics.uci.edu/ml/)
[Programmable API](https://www.programmableweb.com/)

## Survey:
- A/B testing
- Multi-armed bandit: are not split evenly down 50/50. but 2 phases would include a small % of random events. e.g. Pandora, training model while most of the time would use the data that you know works best (90%)
- Conjoint Analysis: show a few options and make very specific selections (e.g. choose your own computer build). Conjoin [using python](https://github.com/Herka/Traditional-Conjoint-Analysis-with-Python/blob/master/Traditional%20Conjoint%20Analyse.ipynb)

#L6 - 03-14-2017

## Regression Analysis

terminology from statistics to AI world
- predicators, covariates = x variables, features, inputs
- outcomes, dependent variable = y variables, outputs
- estimator.score(X, y) => multiple x predictors would be a df or matrix, y is a single-column df
- co-efficent printed from model is the slope
- in machine learning, categorical data needs to be treated as dummy variables because ML doesn't understand string values

### multicolinearity
- when you have two factors with high colinearity, you usually want to take the one with the higher R-square value and drop the other one

#L7 - 03-23-2017

linear regression (OLS) => y = C + w(var1) + w(var2)....

MSE = *mean squared error*, to see how correct a LM line is and if the data requires more normalization
to normalize and minimize error: SGD to calc better line. Use SGD to more efficiently minimize error.

## model is too simplistic
bias in Linear regression = model is too simplistic
adding polynomials (exponents) may solve bias but cause the problem of variance where the model is overfitting by trying to catch all outliers

high bias = underfitting, high error for local data and total data

## model does not generalize
high variance = overfitting, may not generalize, good for local data but high error rate for total data

to fix or account for high-variance, do cross-validation (k-fold):
cross-validation = break your sample data into random and equal subset of samples, run different models which will give you a "total error" which is how your model will apply in real world not just your sample

## regularized method for model that is too complex.
- Lasso (most commonly used for images)
- Ridge (most commonly used for almost all circumstance)

alpha = how much a model penalize complexity or how complex a model is
low alpha = penalize less for complexity
high alpha = penalize a lot for complexity

#L8 - 03-28-2017

## Classification

Another area of machine learning. Unlike regression, it is to deal with true/false or binomial problems. Classification is trying to draw a line to separate the data. Regression is drawing a line through the data.
- Binary classification - true/false
- Multiclass classification - different categories

To determine whether or target/outcome/Y variable can be ordered mathematically or numerically

Exercise 1:
1. classification
2. regression
3. regression
4. clustering
5. classification

### K Nearest Neighbors (KNN)
- doesn't work well with high dimensional data (i.e. lots of X variables)
- to find best classification for a point based on its distance to a group of closest points
- two ways of weighting the distance: 'uniform' (just look at the K closest), 'distance' (look and weight the K closest in order of proximity). 'distance' is usually the better choice to use

### SVM/SVC support vector
- alternative to KNN
- flaw is that this assumes all your data are nicely separated with a margin/boundary between each class
- too many downsides so it slowly falling out of favor in preference to decision trees etc

### optimize
- use `GridSearchCV` for cross validation. mean-test-score and std-test-score are the most important metrics to note

#L9 - 03-30-2017

## Logistic Regression

- actually a classification algorithm
- p(y|x) : what is the probability of Y given X
- linear regression tends to rely on odds instead of probability

### predicting odds
- statsmodel only predicts binomial answers (2 categories). sklearn can do more than two categories.
- AUC of 1 = perfect model, higher the ROC curve the better the model/results
- Recall = how many times you can pull out positive result
- [metrics from scikit learn](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)

### slide 6 exercise:
1. KNN is used to predict categories; linear regression is used to predict a numerical value
2. data is somewhat continuous (like dogs, cats weight vs height)

### slide 21 exercise:
roll a 6 on a die
probability = 1/6
odds = 1:5 (1 chance of success, 5 chances of fail)
log odds = log(1/5)

### slide 47 exercise:
1. False negative is worst = someone has cancer but told not having cancer.
2. False Positive (wonâ€™t pay but will) results in highest actual cost. False Negative (will pay but won't) results in potential cost of customer lost
3. False Positive = worst. thinking it's spam but it's not. False Negative, thinking it's not spam but it is.

#L10 - 04-04-2017

- Cross-validation is needed to evaluate variance rate. Doesn't cross-validated well means a variance problem.
- *Accuracy Paradox* can cause Accuracy be a bad metric to judge your model. To solve this paradox, use *Precision* and *Recall*. Precision is about false hits (type I error). Recall is about misses (type II error).
- Recall is looking at True Class aka true values (TP/P). Precision is looking at predicated class / true class (TP/TP+FP).

### slide 40 exercise

1. Lenders would be more interested for the TPP.
2. "You ran a linear regression. Your R-squared is moderate & MSE is high." => Bias problem - add polynomials or features
"Training well on local but now doesn't generalize" => Variance problem - tuning regularization params, try a smaller set of features if possible

### slide 58 exercise
Precision = 13/(13+8) = 0.619
Recall = 13/(13+7) = 0.65

#L11 - 04-06-2017

## Clustering
- For clustering, the groups are the prediction.
- Silhouette score to determine best kind of clustering model
http://scikit-learn.org/stable/modules/clustering.html

### K-means clustering
- centroid is the mean of each cluster group
https://www.naftaliharris.com/blog/visualizing-k-means-clustering/

### DBSCAN clustering
- tuning params: epsilon, min sample size
https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/

### Agglomerative clustering
- slower, better for medium to smaller dataset

#L12 - 04-11-2017
### Decision trees
- can be both regression or classification depending on how it's used

### slide 19 exercise
1. importance of different variables in regression model:
scaling coefficients and see how that affects each X variables' impact on the Y or target variable.
R-squared values.
2. importance of different variables in classification model:
for loop through the columns and remove step by step to see how much that affects the accuracy. that is one way of estimating the importance but there are not really perfect way to judge this. decision tree helps.

### decision tree model
- *gini impurity* shows which variable is more important to your variable outcome
- "feature_importances" from decisionTree algorithm signals the most predictive variables for classifying your data
- _max_depth_ param will help tune the decision tree to not overfit your data
- cross-validation and tuning help prevent tree overfitting.
- *Random forest model*: ensembling, is a simple and good way to avoid overfitting. It is similar to k-fold. Takes a lot of overfitting trees to reduce variance. The downside is that you can't really visualize random forest which makes it hard to see what is happening. Another downside is that it takes a longer time to train and a lot more computational power. For simpler problems, use a single model instead.
- *ensemble forest model* (aka random forest): warm_start param will help make the model run faster 


- Naive Bayes
http://sebastianraschka.com/Articles/2014_naive_bayes_1.html
